# SentencePiece Tokenizer

This directory contains a SentencePiece Tokenizer trained on the C4 dataset. This tokenizer is intended for use with LLM training. It was trained by Google for MLPerf using [these instructions](https://github.com/sgpyc/training/blob/paxml-llm-draft/large_language_model/paxml/utils/generate_spm.md). The tokenizer was downloaded from the Google Cloud bucket `gs://mlperf-llm-public2/vocab/c4_en_301_5Mexp2_spm.model` on 6/13/2022. 
